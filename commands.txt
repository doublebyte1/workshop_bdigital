//download keys
https://drive.google.com/a/bdigital.org/file/d/0B5kkho5DSzlyanBkUzgtRWh2OVE/view

//connect by ssh
ssh hadoop@ec2-52-17-172-105.eu-west-1.compute.amazonaws.com -i ~/.aws/worshop-bdigital.pem

//view the data

stored on this bucket
s3://workshop-bdsd/accidents/

aws s3 ls s3://workshop-bdsd/accidents/

https://s3-eu-west-1.amazonaws.com/workshop-bdsd/accidents/accidents_sample.csv

cat accidents_sample.csv

//import the data
CREATE EXTERNAL TABLE accident_import(
  Any  INT, mes  INT, dia  INT, C_HORA_TRUCADA   DOUBLE, D_COORD_GEO_IMPACTE  STRING)
row format serde 'com.bizo.hive.serde.csv.CSVSerde'
with serdeproperties(
"separatorChar" = "\;",
"quoteChar" = "\"")
stored as textfile
LOCATION 's3n://workshop-bdsd/accidents/'
tblproperties ("skip.header.line.count"="1");

describe accident_import

//create a table with correct types
CREATE TABLE accident_data(
  Any  INT, mes  INT, dia  INT, C_HORA_TRUCADA   DOUBLE, D_COORD_GEO_IMPACTE  STRING);
  
insert into table accident_data select * from accident_import; 

//create table for pig (id)
create table accident_data_id as
select reflect("java.util.UUID", "randomUUID") as id, * from accident_data;

geohive -e 'select * from accident_data_id where D_COORD_GEO_IMPACTE is not null' > accident_data.tsv

hadoop fs -mkdir /data
hadoop fs -put accident_data.tsv /data
hadoop fs -cat /data/accident_data.tsv

wget https://s3-eu-west-1.amazonaws.com/workshop-bdsd/recover_geography.pig


create external table accident_data_wgs84 
(  id string, Any  INT, mes    INT, dia     INT, C_HORA_TRUCADA  DOUBLE, D_COORD_GEO_IMPACTE   STRING, id2 string, lon DOUBLE, lat DOUBLE)
  ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n'
  location '/data/data_wgs84';

create external table accident_data_utm 
(  id string, Any  INT, mes    INT, dia     INT, C_HORA_TRUCADA  DOUBLE, D_COORD_GEO_IMPACTE   STRING, id2 string, lon DOUBLE, lat DOUBLE)
  ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n'
  location '/data/data_utm';

create external table accident_data_pol 
(  id string, Any  INT, mes    INT, dia     INT, C_HORA_TRUCADA  DOUBLE, D_COORD_GEO_IMPACTE   STRING, id2 string, lon DOUBLE, lat DOUBLE)
  ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n'
  location '/data/data_pol';

  //merge utm tables  
 create table utm_data_all as select * from accident_data_utm; 
 insert into table utm_data_all select * from accident_data_pol;

 geohive -e 'select id,lon,lat from utm_data_all where lon is not null' > utm_export.tsv
  
 psql -h bdigitaldb.celqzuwfokoe.eu-west-1.rds.amazonaws.com -p 5432 -U workshop workshop_bdigital -W
  
 select * from spatial_ref_sys where srid=5554;

create table coords(id varchar, lon double precision, lat double precision);
\COPY coords FROM 'utm_export.tsv';

alter table coords add column utm geometry;
alter table coords add column wgs84 geometry;

select updategeometrysrid('coords','utm', 5554);
select updategeometrysrid('coords','wgs84', 4326);

CREATE INDEX idx_coords_utm ON coords USING GIST (utm);
CREATE INDEX idx_coords_wgs84 ON coords USING GIST (wgs84);

update coords set utm=ST_SetSRID(ST_MakePoint(lon,lat), 5554);
UPDATE coords set wgs84=(ST_Transform(utm, 4326));

//view coordinates

\COPY (select id, ST_ASGeoJson(wgs84) from coords) TO 'coords_data.json';

hadoop fs -mkdir /data/coords_data
hadoop fs -put coords_data.json /data/coords_data

CREATE EXTERNAL TABLE coords_data_tmp (id STRING, json STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
LOCATION '/data/coords_data/';

//create a table with an instantiation of the grid geometry
CREATE TABLE coords_data AS
SELECT id, ST_GeomFromGeoJson(json) as geom from coords_data_tmp;

create table all_geometry as
select 
  accident_data_id.id,Any , mes , dia  , C_HORA_TRUCADA , D_COORD_GEO_IMPACTE     
  , ST_X(geom) as lon, ST_Y(geom) as lat from accident_data_id join coords_data ON (accident_data_id.id=coords_data.id);

insert into table all_geometry
  select   id, any  , mes   , dia   , C_HORA_TRUCADA  , D_COORD_GEO_IMPACTE , lon, lat from accident_data_wgs84;
  
  
select concat("{\"index\":{\"_index\":\"accidents_sample\",\"_type\":\"accident\",\"_id\":", rowsequence() , "}} \n") ,  "loc", array(cast(lon as double),cast(lat as double)) from all_geometry;
    
//export data
geohive -e 'select concat("{\"index\":{\"_index\":\"accidents_sample\",\"_type\":\"accident\",\"_id\":", rowsequence() , "}} \n") ,  "loc", array(cast(lon as double),cast(lat as double)) from all_geometry;' > all_geometry.json

http://52.16.16.8/#/dashboard/accident_dash?_g=%28refreshInterval:%28display:Off,section:0,value:0%29,time:%28from:now-2y,mode:quick,to:now%29%29&_a=%28filters:!%28%29,panels:!%28%28col:5,id:weekly_acidents,row:3,size_x:5,size_y:4,type:visualization%29,%28col:1,id:accidents_hash,row:3,size_x:4,size_y:4,type:visualization%29,%28col:10,id:Circle_geohash,row:3,size_x:3,size_y:4,type:visualization%29,%28col:1,id:line_chart,row:1,size_x:12,size_y:2,type:visualization%29%29,query:%28query_string:%28analyze_wildcard:!t,query:%27*%27%29%29,title:accident_dash%29


 
 